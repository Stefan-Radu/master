\chapter{Background Information} % TODO obv change this

%TODO other things to add:
%* blocks
%* opcodes
%* CFG

In this chapter we will cover concepts relevant for the rest of the paper. %TODO continue

%TODO might be a good idea to introduce the concept of malware (with categories and all) and obfuscation before reverse engineering

\section{Reverse Engineering} % TODO obv change this
% TODO get a glossary going
% TODO glossary list:
% * reverse engineering

As stated on \emph{Wikipedia}: \emph{``Reverse engineering is a process or method through which one attempts to understand through deductive reasoning how a previously made device, process, system, or piece of software accomplishes a task with very little (if any) insight into exactly how it does so''} \cite{re_wiki}.
% TODO maybe give more context into historical examples of reverse engineering and how they benefited society: hierogliphs, russian plane, telescope, IBM pc, etc.

% TODO define malware

In this work we will focus on reverse engineering pieces of software in the context of performing security research or malware analysis. Typically, in such contexts, the goal is to understand the behaviour of a piece of software, without having access to its source code. Depending on the type of analysis, a reverse engineer might have different approaches. 

They might focus on gaining a comprehensive understanding of specific parts of the software in order to identify weaknesses, or more commonly named \emph{vulnerabilities}. The analysis could be restricted to specific parts because for multiple reasons which include, but are not limited to: the full program being to big to justify performing a full analysis, or the existence of prior knowledge which gives higher priority to the analysis of certain code regions. With the knowledge obtained from RE, the engineer can identify and prove the existence of attack vectors on a system that is running this software. They might then write a report which covers the risks to which entities running the software are exposed, describing in detail the finding and, eventually exemplifying how an attacker might abuse the discovered vulnerabilities. In this case, the end goal is to initiate the process of patching the vulnerable piece of software. % TODO might want to cite here

In other instances, the engineers might perform a full and comprehensive analysis of piece of software. This is typically done when dealing with malware. The malware analyst will first try to determine if the piece of software is in fact malicious or not. If the code is malicious, it is important to determine its behaviour, how it interacts with the system, or with outside entities (possibly by creating network traffic). Further, analysts might study and document novel techniques employed by attackers. They might also integrate the newly find malware into a detection system to prevent future uses of the respective malware \cite{malware_crowdstrike}.

Regardless of the goal, RE falls in one of two broad categories which determine the typical approach and the tooling used: \textbf{static analysis} and \textbf{dynamic analysis}.

\section{Static Analysis}

Static analysis represents the multitude of techniques employed to analyse a program without executing it. These techniques range in difficulty and complexity starting from reading source code, to reading assembly, attempting to decompile the binary and ultimately to using very advanced tools and theoretical knowledge such as \gls{SE} engines, SMT solvers or formal methods. %TODO I am not sure anylonger if SE should be included here or not

In its most basic form, static analysis is equivalent with reading the source code in order to understand what the program does. However, in the context of this paper, we are dealing with binary files, compiled to machine code. The source code is not available to us, so we must resort to other analysis techniques. One option is to convert the machine code into the human readable form, called assembly. The process is known as machine code disassembly. Assembly code is typically very hard to understand for humans, but from it the logic of the program can be successfully recovered, given enough time and effort. One advantage of static analysis through reading assembly is that the entry barrier is not high in terms of the tooling required. A very basic tool such as \cc{objdump} \cite{TODO} can be enough for simple programs, but most likely a more feature rich tool such as \cc{radare2}/\cc{cutter} \cite{TODO} might be more suitable, as these are able to display basic blocks and how all such basic block(BB TODO) relate to each other and form the Control Flow Graph(CFG TODO).

Reverse engineers typically default to more advanced static analysis tools, such as IDA or Ghidra. These tools feature a suite of functionalities, out of which, probably the most prominent is the \emph{decompiler}. Compilation is the process of converting source code into machine code. Decompilation is the opposite: the process of converting machine code, back into source code. Compared to the disassembly process, which is deterministic and corresponds exactly to the assembly process, the decompilation process will almost never yield back the original source code. This is the case because a lot of information useful to programmers, but useless for the CPU(TODO) is lost during the compilation process. This information includes, but is not limited to: variable and function names, type information, custom defined data types such as structures, or specific language features. This is why decompilers will always output an approximation of the original code. 

Let us consider a concrete example and consider Listings \ref{code:decompilation-original}, \ref{code:decompilation-1}, \ref{code:decompilation-2}. Listing \ref{code:decompilation-original} contains the implementation of the function \cc{add}, which takes the end of linked list and a value, and creates a new node in the list with that value, also taking the necessary steps to update the list accordingly. The code is part of a slightly larger \cc{C} program, which we compiled and imported into Ghidra. Looking at Listing \ref{code:decompilation-1} we can see the decompilation of the exact code in the previously mentioned listing. It is immediately obvious that: the type information related to \cc{node} structure is completely lost and that the original function was inlined by the compiler. As a result, the decompilation is an obfuscated version of the original code. This is a well known fact and advanced tools such as Ghidra offer various features, which the reverse engineer can utilise in order to remove part of the obfuscation. Listing \ref{code:decompilation-2} contains the same segment of decompiled code after a minimal amount of manual intervention, which includes: variable renaming, custom type creation and type updates. Clearly, it is a lot more human readable and bears a closer resemblance to the original code in Listing \ref{code:decompilation-original}. 

Decompilers and disassemblers are very powerful tools, which aid significantly in the process of static analysis. However, these tools have their shortcomings as highlighted above. Moreover, there are certain program behaviours which cannot, or are significantly harder to understand only by \emph{looking} at the code.

% Found a paper which I can use to add more about this :D
% see static analysis techniques 2015

% TODO mention techniques which hinder static analysis

% TODO set vspace / margin before listings
\begin{lstlisting}[language=c, label={code:decompilation-original}, caption={TODO}]
void add(lnode** node, int v) {
    // allocate memory for a new node in the liked list
    lnode* new_node = (lnode*) malloc(sizeof(lnode));
    new_node->val = v; // set the value
    if (*node != NULL) {
        (*node)->nxt = new_node; // link to the new node from the end of the list
        *node = new_node; // move the list end to the new node
    } else {
        *node = new_node; // TODO fix comments set it as the end of the list, it it is the first one
    }
}
\end{lstlisting}

% TODO i'd like to try to get two columns in here
\begin{lstlisting}[language=c, label={code:decompilation-1}, caption={TODO}]
piVar1 = (int *)malloc(0x10);
*piVar1 = iVar3;
if (piVar5 != (int *)0x0) {
    *(int **)(piVar5 + 2) = piVar1;
}
piVar5 = piVar1;
if (piVar4 == (int *)0x0) {
    piVar4 = piVar1;
}
\end{lstlisting}

\begin{lstlisting}[language=c, label={code:decompilation-2}, caption={TODO}]
new_node = (node *)malloc(0x10);
new_node->val = v;
if (last_node != (node *)0x0) {
    last_node->nxt = new_node;
}
last_node = new_node;
if (root == (node *)0x0) {
    root = new_node;
}
\end{lstlisting}

\section{Dynamic Analysis}

As described by T. Ball in his 1999 paper \cite{concept_of_da_1999}, \emph{``dynamic analysis is the analysis of a running program''}. This type of analysis is desirable in different situations where static analysis could not extract sufficient information, or when acquiring extra information depends the program to be running %TODO i could give more examples here, relating to why static analysis cannot work in certain situations. also should mention for the static analysis part exactly this. probably will need to talk before RE about obfuscation techniques. OR NOO!! I can talk right after, in order to present some means to prevent static analysis
Dynamic analysis is an umbrella term which covers many powerful techniques used for program analysis. A taxonomy of these techniques has been presented in a comprehensive survey by Ori et al. in 2019 \cite{da_survey_2019}. We will shortly cover some of them.

\subsection{Debugging}

% TODO define somewhere that we'll be referring to a reverse engineer / malware analysit as "the analyst" throughout this work

Debugging is a very well known technique, especially popular among developers who use it mainly to identify bugs or errors in their code. It is however a very effective and reliable form of analysing unknown programs (eg. malware). Also called \emph{single stepping}, it involved using a tool, intuitively called a \emph{debugger}, in order to run the program one instruction at a time. After each instruction, the analyst can inspect the state of the registers, the memory and what instructions follow. This process can also help in determining any relevant changes in the operating system itself, caused or related to the debugged program.

Debuggers use the CPU's trap flag in order to trigger an interrupt after each instruction, or only certain desired instructions. The interrupt causes a context switch from the execution of the debugged program to the debugger. To continue execution, the trap flag is set again and the context switches back to the program. The high number of context switches means that debugging is a very resource intensive analysis technique. It is also very easy to detect by the running malware, which can check the state of the trap flag and hide its behaviour in case it is debugged \cite{da_survey_2019}.

\subsection{Function Call Analysis}

% TODO explain what a syscall is?

Any type of meaningful action that a malware can take, will ultimately rely on system calls. It can be the case that these system calls are performed through function calls from an external library, such as the standard \cc{libc} library, or from an internally defined function. Analysing function calls, the state of the program before, during and after the function call, as well as the parameters used can provide valuable information about the behaviour of the analysed program. Techniques for approaching this goal vary. For instance, we could use command line (cli TODO) programs such as \cc{strace}, or \cc{ltrace}, which track system calls and library calls respectively. We could also use more advanced techniques, such as function hooking. An analyst can extract more information from a function call by \emph{hooking} (i.e. linking) a piece of code to the targeted function. What will happen is that upon the function call, the \emph{hooked} code will run as well, which can just print debugging messaged to inform the analyst that the function has just been called, or access the state of the program at that time and save it for further inspection. \cite{da_survey_2019}

Function calls can also be used as a powerful side-channel. More specifically, one can monitor the amount of \cc{calls} which have been made since reference point in order to determine if progress has been made or not in the execution. Let's consider Listing \ref{code:ltrace}. We're running the \cc{crackme} program through \cc{ltrace} to monitor function calls, with a randomly chosen input string. We notice a length check with \cc{strlen} at Line \ref{code:ltrace-1}, after which the program crashes. By selecting the correct input length of $70$ bytes, we can pass the length check at Line \ref{code:ltrace-2}. This \cc{crackme} is a particularly good example for using this technique, because it is heavily obfuscated. We cannot effectively use static analysis on this binary, so employing dynamic analysis techniques enables us to make progress and recover the secret \cite{crusu_relabs}.

% TODO explain what a crackme is 

\begin{lstlisting}[caption={TODO}, label={code:ltrace}]
>_ python -c "print('a' * 42)" | ltrace ./crackme
memset(0x8625ae8, '\0', 10000)                         = 0x8625ae8
fgets("aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"..., 10000, 0xf22e9700) = 0x8625ae8
strlen("aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"...)          = 43 @\label{code:ltrace-1}@
puts("WROOONG!"WROOONG!)                               = 9
exit(1 <no return ...>)
+++ exited (status 1) +++

>_ python -c "print('a' * 70)" | ltrace ./crackme
memset(0x8625ae8, '\0', 10000)                         = 0x8625ae8
fgets("aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"..., 10000, 0xedcf4700) = 0x8625ae8
strlen("aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"...)          = 71
strstr("aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"..., "zihldazjcn") = nil @\label{code:ltrace-2}@
puts("WROOONG!"WROOONG!)
exit(1 <no return ...>)
\end{lstlisting}

\subsection{Fuzzing}

\subsection{Dynamic Taint analysis}

Dynamic Taint analysis is a technique used to track data flow from sources to sinks. In order to achieve this goal, data considered important is given a label (\emph{a taint}), based on a \emph{taint introduction policy}. Typically, we would taint untrusted user input or data arriving over the network. This \emph{tainted} data is propagated through the system based on execution and how the code interacts with the data at the opcode level. When an operation is performed on tainted data, memory locations used during the respective operation are also tainted, based on a \emph{taint propagation policy}. Some memory areas, or code sections are also marked as \emph{sinks}. When tainted data arrived at a sink, the path it took through the code can be traced back. In the context of malware analysis, the flow of tainted data is valuable, as it gives valuable insights about the ways the malware interacts with the user and the operating system. Taint analysis is also valuable for exploit detection, and was initially used specifically for this goal. By tainting untrusted user input one can detect unusual data flows and detect attempts at exploiting a system. In such cases, a \emph{taint checking policy} might be used to determine further behaviour (e.g. halting execution) \cite{da_survey_2019} \cite{all_about_taint_2010}.

\subsection{Symbolic Execution}

\gls{SE} is a powerful program analysis technique, and one of the core techniques which the idea of this paper is based on. As such, we will cover \gls{SE} in more detail compared to the other dynamic analysis approaches.

% TODO bat campii efectiv, trebuie sa reformulez
\gls{SE} is typically discussed in relation with \emph{\gls{CE}}. \gls{CE} is the formal term for what we refer to as normal program execution. That is, executing a program with a concrete input until the end of a single execution path. When every possible external value (user input, response from a system call, return value of a function) has a concrete value, we're dealing with \gls{CE}.
Let us consider Listing \ref{lst:fizzbuzz}. The value of the argument \cc{c} is given by the caller of the function \cc{fizzbuzz}. If we consider a concrete value of $7$ for \cc{c}, we expect the program to print the same value $7$ at the standard output, after executing Line \ref{code:fizzbuzz-1}. We can test this hypothesis by running the program, passing the respective value to the function and inspecting the printed value. This is concrete execution.

\lstinputlisting[language=c, label={lst:fizzbuzz}, caption={TODO}]{./code/fizz-buzz.tex}

In contrast with \gls{CE}, with \gls{SE} we can explore all possible paths of execution. Moreover, for each path we will have a logical condition which precisely describe the values of the inputs which will lead the execution on that specific path.

in order to determine what classes of inputs lead to which execution paths.

The key aspect of \gls{SE} is the use of symbolic values, as opposed to concrete values. Initially, the symbolic values are unconstrained, meaning that they can represent any possible input value associated to that type. The program is executed in a controlled environment by a \gls{SE} engine. The engine keeps track of a logic formula which describes the constraints that the input must satisfy, in order to follow each specific execution path that is being tracked. It also keeps trace of the symbolic memory store, which keeps track of symbolic values and memory areas, and the expressions or concrete values which these hold. As the program executes, each conditional branch splits the symbolic state into two and adds new constraints to each branch, based on the respective conditional.

\subsection{Mixed Approaches}

% TODO this paragraph does not fit here AT ALL!
% TODO find a way to repurpose it
% Typically, when running unknown software the researcher or analyst is exposing his system to the risk of being compromised. This is especially prevalent when we are dealing with potential malware. For these situations specific techniques have been devised in order to protect the host system from being infected, while still having an environment where the unknown software can be executed and its interactions with the system monitored and then analysed. Such environments are called sandboxes, and isolate the suspicious software from the main OS(TODO), in such a way that the host of the system, as well as the network are safe from infection, or compromise.


%Incep sa scriu la disertatie asta e prima linie.
%Should be english though.

%List of ideas and sections to cover:

%* Reverse engineering
%* What/why is obfuscation
%* Obfuscation techniques
%* Vms
%* Vm-based obfuscation
%* State of the art VM-based obfuscation
%* Techniques for reverse engineering vm-based obfuscation
%* Novel approaches - Symbolic execution-based tool
%* On Symbolic execution
%* Popular Symbolic execution tools
    %* Angr
    %* Miasm
    %* comparison
%* Implementation in both
    %* comparison
%* Future directions
